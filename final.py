import requests
import json
import pandas as pd
import sys
import os
import time
from lxml import etree
import re
import msvcrt

NO_secret_json = False  #é è¨­æœ‰SECRET.json

def set_working_directory():
    # ç²å–åŸ·è¡Œæª”æ¡ˆçš„è·¯å¾‘
    exe_path = sys.argv[0]
    # è½‰æ›ç‚ºçµ•å°è·¯å¾‘
    exe_dir = os.path.abspath(os.path.dirname(exe_path))
    # è¨­ç½®å·¥ä½œç›®éŒ„
    os.chdir(exe_dir)

#LoadingBar
def ANSI_string(s, color=None, background=None, bold=False):
    colors = {
        'black': '\033[30m',
        'red': '\033[31m',
        'green': '\033[32m',
        'yellow': '\033[33m',
        'blue': '\033[34m',
        'magenta': '\033[35m',
        'cyan': '\033[36m',
        'white': '\033[37m',
        'reset': '\033[0m'
    }
    
    background_colors = {
        'black': '\033[40m',
        'red': '\033[41m',
        'green': '\033[42m',
        'yellow': '\033[43m',
        'blue': '\033[44m',
        'magenta': '\033[45m',
        'cyan': '\033[46m',
        'white': '\033[47m',
        'reset': '\033[0m'
    }
    
    styles = {
        'bold': '\033[1m',
        'reset': '\033[0m'
    }
    color_code = colors[color] if color in colors else ''
    background_code = background_colors[background] if background in colors else ''
    bold_code = styles['bold'] if bold else ''

    return f"{color_code}{background_code}{bold_code}{s}{styles['reset']}"

def getData_loading_bar(duration, k):
    total_ticks = 20
    for i in range(total_ticks + 1):
        if i == total_ticks:
            progress = ANSI_string('[',bold=True) + ANSI_string('=',color='cyan') * total_ticks + ANSI_string('=',color='cyan') + ANSI_string(']',bold=True)
        else:
            progress = ANSI_string('[',bold=True) + ANSI_string('=',color='blue') * i + ANSI_string('>',color='blue') + ' ' * (total_ticks - i) + ANSI_string(']',bold=True)
        sys.stdout.write('\r' + progress + f' æŠ“å–ç¬¬{k}ç­†è³‡æ–™ä¸­ï¼Œè«‹ç­‰å¾…' + '.' * (i % 4) + ' ' * (4 - i % 4))
        sys.stdout.flush()
        time.sleep(duration / total_ticks)
    sys.stdout.write('\n')

def waiting_loading_bar(duration):
    total_ticks = duration
    for i in range(total_ticks + 1):
        if(i==total_ticks):
            sys.stdout.write('\r' + ANSI_string(f'ç­‰å¾…å®Œç•¢ï¼Œé–‹å§‹åŸ·è¡Œä¸‹ä¸€æ­¥',bold=True))
            sys.stdout.flush()
        else:    
            sys.stdout.write('\r' + ANSI_string(f'è«‹ç­‰å¾…{duration-i}ç§’',bold=True) + '.' * (i % 4) + ' ' * (4 - i % 4))
            sys.stdout.flush()
            time.sleep((duration / total_ticks))
    sys.stdout.write('\n')

NOTION_TOKEN = ""
PAGE_ID = ""
DATABASE_ID = ""
#NotionAPI

set_working_directory()
# æª¢æŸ¥æ˜¯å¦å­˜åœ¨ SECRET.json æª”æ¡ˆ
secret_json_path = os.path.join(os.getcwd(), 'SECRET.json')
if os.path.exists(secret_json_path):
    # è®€å– JSON æª”æ¡ˆ
    with open(secret_json_path) as file:
        data = json.load(file)
    # å–å¾— integration å’Œ Database è³‡è¨Š
    NOTION_TOKEN = data.get('notion_id')
    PAGE_ID = data.get('page_id')
    file.close()
else:
    NO_secret_json = True


class NotionClient():
    def __init__(self):
        self.notion_key = NOTION_TOKEN
        self.default_headers = {'Authorization': f"Bearer {self.notion_key}",
                                'Content-Type': 'application/json', 'Notion-Version': '2022-06-28'}
        self.session = requests.Session()
        self.session.headers.update(self.default_headers)    

    def create_database(self, data):
        url = "https://api.notion.com/v1/databases"
        response = self.session.post(url, json=data)
        return response.json()

    def create_page(self, data, databaseID):
        url = "https://api.notion.com/v1/pages"
        payload = {"parent": {"database_id": databaseID}, "properties": data}
        response = requests.post(url, headers=self.default_headers, json=payload)
        return response.json(),response.status_code

def CreateDatabase(page_id,author):
    notion_client = NotionClient()
    print("å»ºç«‹databaseä¸­ï¼Œè«‹ç­‰å¾…")
    waiting_loading_bar(1)
    # Create a database with some properties
    data = {
        "parent": {
            "type": "page_id",
            "page_id": page_id
        },
        "icon": {
            "type": "emoji",
                "emoji": "ğŸ“–"
        },
        "title": [
            {
                "type": "text",
                "text": {
                    "content": f"{author}",
                    "link": None
                }
            }
        ],
        "properties": {
            "æ›¸å": {
                "title": {}
            },
            "æ›¸æœ¬å°é¢": {
                 "files": {}
            },
            "æ›¸æœ¬é€£çµ": {
                 "url": {}
            },
            "ISBN": {
                "rich_text": {}
            },
            "ä½œè€…": {
                "rich_text": {}
            },   
            "å‡ºç‰ˆç¤¾": {
                "rich_text": {}
            },
            "å‡ºç‰ˆæ—¥æœŸ": {
                "date": {}
            }    
        }
    }
    catches_create_response = notion_client.create_database(data)
    json_str = json.dumps(catches_create_response, indent=2)
    # # å¯«å…¥åˆ°æ–‡ä»¶
    # with open('catches_database.json', 'w', encoding='utf-8') as f:
    #     f.write(json_str)
    # f.close()
    #print(json_str)
    catches_dict = json.loads(json_str)
    create_database_fail_statement = ""
    if ('status' in json_str) and catches_dict['status']==400:
        create_database_fail_statement = 'Databaseå»ºç«‹å¤±æ•—: ç„¡æ•ˆçš„Page IDï¼Œè«‹å†æ¬¡ç¢ºèªé€£çµç„¡èª¤'
        print(ANSI_string(s=create_database_fail_statement,color='red'))
        return ""
    elif('status' in json_str) and catches_dict['status']==401: 
        create_database_fail_statement = 'Databaseå»ºç«‹å¤±æ•—: ç„¡æ•ˆçš„æ•´åˆå¯†ç¢¼ï¼Œè«‹ç¢ºèªé é¢æ˜¯å¦æˆåŠŸé€£çµåˆ°æ‚¨çš„integrationï¼Œæˆ–æ˜¯å†æ¬¡æª¢æŸ¥æ•´åˆå¯†ç¢¼æ˜¯å¦æœ‰èª¤'
        print(ANSI_string(s=create_database_fail_statement,color='red'))
        return ""
    elif('status' in json_str) and catches_dict['status']==404:
        create_database_fail_statement = 'Databaseå»ºç«‹å¤±æ•—: æ­¤é é¢ä¸å­˜åœ¨ï¼Œæˆ–æ˜¯æ‚¨æœªæˆåŠŸé€£çµè‡³æ‚¨çš„integrationä¸Š'
        print(ANSI_string(s=create_database_fail_statement,color='red'))
        return ""
    else:
        database_ID = catches_dict["id"]
        return database_ID

def NormalizeDate(date):
    if(not ('/' in date)):
        date = '1000/1/1'  
    date_split = date.split('/')
    year = date_split[0]
    mon =  date_split[1]
    day =  date_split[2]
    if(len(mon)==1):
        mon = '0' + mon
    if(len(day)==1):
        day = '0' + day
    return f'{year}-{mon}-{day}'

def CreatePage(databaseID,title=None,book_img=None,ISBN=None,author=None,publish=None,published_date=None,book_link=None):
    notion_client = NotionClient()
    published_date = NormalizeDate(published_date)
    data = {
        "æ›¸å": {"title": [{"text": {"content": title}}]},
        "æ›¸æœ¬å°é¢": {
                    "id": "V%5E%5Be",
                    "type": "files",
                    "files": [
                        {
                            "name": book_img,
                            "type": "external",
                            "external": {
                                "url": book_img
                            }
                        }
                    ]
                },
        "ISBN": {
                    "rich_text": [
                        {
                            "text": {
                                "content": ISBN,
                            },
                        }
                    ]
                },
        "ä½œè€…": {
                    "rich_text": [
                        {
                            "text": {
                                "content": author,
                            },
                        }
                    ]
                },
        "å‡ºç‰ˆç¤¾": {
                    "rich_text": [
                        {
                            "text": {
                                "content": publish,
                            },
                        }
                    ]
                },
        "å‡ºç‰ˆæ—¥æœŸ": {"date": {"start": published_date, "end": None}},
        "æ›¸æœ¬é€£çµ": {"url":book_link}
    }
    status_code = notion_client.create_page(data = data,databaseID = databaseID)[1]
    if(status_code==200):
        print(ANSI_string(ANSI_string(f'{title}',bold=True)+'ä¸Šå‚³è‡³NotionæˆåŠŸ',color='green'))
    else:
        print(ANSI_string(ANSI_string(f'{title}',bold=True)+'ä¸Šå‚³è‡³Notionå¤±æ•—',color='red'))

def EstablishFullDatabase(keyword,df = pd.DataFrame({'æ›¸å': [], 'æ›¸æœ¬å°é¢':[], 'ISBN': [], 'ä½œè€…':[], 'å‡ºç‰ˆç¤¾':[],'å‡ºç‰ˆæ—¥æœŸ':[], 'æ›¸æœ¬é€£çµ': []})):
    databaseID = CreateDatabase(author=keyword,page_id=PAGE_ID)
    if databaseID != "":
        for i in range(len(df['æ›¸å'])):
            CreatePage(databaseID=databaseID,title=df['æ›¸å'][i],book_img=df['æ›¸æœ¬å°é¢'][i],ISBN=df['ISBN'][i],author=df['ä½œè€…'][i],publish=df['å‡ºç‰ˆç¤¾'][i],published_date=df['å‡ºç‰ˆæ—¥æœŸ'][i],book_link=df['æ›¸æœ¬é€£çµ'][i])

#GetBookData
headers = {"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
               "AppleWebKit/537.36 (KHTML, like Gecko)"
               "Chrome/63.0.3239.132 Safari/537.36"}

def extract_book_id(url):
    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼å¾URLä¸­æå–æ•¸å­—éƒ¨åˆ†
    match = re.search(r'/products/(\d+)', url)
    if match:
        return match.group(1)
    else:
        return None

def get_BookTitle(html):
    title_x = html.xpath('/html/body/div[4]/div/div[1]/div/div/div/div[1]/h1/text()')
    #title_x = html.xpath('/html/body/div/div/div/div/div/div/div/h1/text()')
    if(title_x != []):
        title = title_x[-1]
        return title
    else:
        title_x = html.xpath('/html/body/div[4]/div/div[1]/div[2]/div[1]/h1/text()')
        if(title_x != []):
            title = title_x[-1]
            return title
        else:
            return 'æœªæ‰¾åˆ°è³‡æ–™'

def get_ISBN(html):
    ISBN_x = html.xpath('/html/body/div[4]/div/div/div[1]/div/div/ul[1]/li[1]/text()')
    #ISBN_x = html.xpath('/html/body/div/div/div/div/div/div/ul/li[1]/text()')
    if(ISBN_x != []):
        has_isbn = any(item.startswith('ISBN') or item.startswith('æ¢ç¢¼') for item in ISBN_x)
        has_Eisbn = any(item.startswith('EISBN') for item in ISBN_x)
        if(has_isbn or has_Eisbn):
            ISBN = ISBN_x[-1].split("ï¼š")[1]
            return ISBN
        else:
            return 'æœªæ‰¾åˆ°ISBN'
    else:
        ISBN_x = html.xpath('/html/body/div[4]/div[3]/div[1]/section[5]/div/ul[1]/li[1]/text()')
        if(ISBN_x != []):
            ISBN = ISBN_x[-1].split("ï¼š")[1]
            return ISBN
        else:
            return 'æœªæ‰¾åˆ°è³‡æ–™'

def get_Author(html):
    author_x = html.xpath("/html/body/div/div/div/div/div/ul/li[contains(., 'ä½œè€…')]/a/text()")
    if(author_x == []):
        author_x = html.xpath("/html/body/div/div/div/div/div/ul/li[contains(., 'ç·¨è€…')]/a/text()")
    if(author_x!=[]):
        if(' ' in author_x[0]):
            author = re.sub(r'\s+', 'ï¼Œ', author_x[0])
            return author
        return author_x[0]
    else:
        author_x = html.xpath("/html/body/div/div/div/div/div/div/div/div/ul/li[contains(., 'ä½œè€…')]/a/text()")
        if(author_x == []):
            author_x = html.xpath("/html/body/div/div/div/div/div/div/div/div/ul/li[contains(., 'ç·¨è€…')]/a/text()")
        if(author_x!=[]):
            if(' ' in author_x[0]):
                author = re.sub(r'\s+', 'ï¼Œ', author_x[0])
                return author
            return author_x[0]
        else:
            return 'æœªæ‰¾åˆ°è³‡æ–™'
        
def get_Publishing(html):
    publish_x = html.xpath('/html/body/div/div/div/div/div/ul/li[contains(., "å‡ºç‰ˆç¤¾")]/a/span/text()')
    if(publish_x != []):
        publish = publish_x[-1]
        return publish
    else:
        publish_x = html.xpath('/html/body/div/div/div/div/div/ul/li[contains(., "å‡ºç‰ˆç¤¾")]/a/text()')
        if(publish_x != []):
            publish = publish_x[-1]
            return publish
        else:
            return 'æœªæ‰¾åˆ°è³‡æ–™'
        
def get_PublishDate(html):
    date_x = html.xpath('/html/body/div/div/div/div/div/ul/li[contains(., "å‡ºç‰ˆæ—¥æœŸ")]/text()')
    if(date_x!=[]):
        return date_x[0].split("ï¼š")[1]
    else:
        date_x = html.xpath("/html/body/div/div/div/div/div/div/div/div/ul/li[contains(., 'å‡ºç‰ˆæ—¥æœŸ')]/text()")
        if(date_x!=[]):
            return date_x[0].split("ï¼š")[1]
        else:
            return 'æœªæ‰¾åˆ°è³‡æ–™'

def get_bookImg(html):
    img_x = html.xpath('/html/body/div[4]/div/div[1]/div[1]/div/div/img/@src')
    if(img_x != []):
        img_link = img_x[0].split('&')[0]
        img_link = img_link.split('?i=')[-1]
        return img_link
    else:
        img_x = html.xpath('/html/body/div[4]/div[1]/div[1]/div/div/div[1]/div[1]/div/img/@src')
        if(img_x != []):
            img_link = img_x[0].split('&')[0]
            img_link = img_link.split('?i=')[-1]
            return img_link
        else:
            return 'æœªæ‰¾åˆ°è³‡æ–™'
    
def get_book_data(url):
    res = requests.get(url,headers=headers, timeout=10)
    bookID = extract_book_id(url)
    if(res.status_code==requests.codes.ok):
        content = res.content.decode()
        html = etree.HTML(content)    
        title = get_BookTitle(html)
        ISBN = get_ISBN(html)
        author = get_Author(html)
        publish = get_Publishing(html)
        date = get_PublishDate(html)
        bookImglink = get_bookImg(html)
        book_data = [title, ISBN, author, publish, date, bookImglink]
        return book_data
    else:
        return 'fail',res.status_code

#GetPageData
def generate_author_url(keyword):   #è¼¸å…¥ä½œè€…å¾Œç”¢ç”Ÿä½œè€…é é¢ä¹‹é€£çµ
    link = "https://search.books.com.tw/search/query/key/"+keyword+"/adv_author/1/"
    print(link)
    return link

def generate_book_url(bookID): #åˆ©ç”¨æ›¸æœ¬IDç”¢ç”Ÿè©²æ›¸é€£çµ
    return "https://www.books.com.tw/products/" + bookID + "?sloc=main"

def get_bookID(book_links): #å–å¾—æ›¸æœ¬IDï¼Œç‚ºç”¢ç”Ÿæ›¸æœ¬é€£çµç”¨
    book_ids = []
    for link in book_links:
        book_id_match = re.search(r'/item/([^/]+?)/', link)
        if book_id_match:
            book_id = book_id_match.group(1)
            book_ids.append(book_id)
        else:
            print(f"No book ID found in link: {link}")
    return book_ids

headers = {"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
               "AppleWebKit/537.36 (KHTML, like Gecko)"
               "Chrome/63.0.3239.132 Safari/537.36"}

def page_crawel(page_url):
    res = requests.get(page_url,headers=headers)
    if(res.status_code==requests.codes.ok):
        print("é€£æ¥æˆåŠŸ!")
        print("æŠ“å–è³‡æ–™...")
        
        content = res.content.decode()  #è§£ç¢¼ç¶²é 
        html = etree.HTML(content)
        
        book_link = html.xpath('/html/body/div/div/div/div/div/div/div/div/h4/a/@href') #å°‡æ­¤é é¢æ‰€æœ‰æ›¸ç±é€£çµéƒ½æ”¾é€²book_link[]
        bookIDs = get_bookID(book_link) #æŠŠIDå¾é€£çµæ“·å–å‡ºä¾†
        
        #ç”¢ç”Ÿæ›¸ç±é€£çµé™£åˆ—ï¼Œå¯ä»¥éæ­·é€™å€‹é™£åˆ—ä¾†é”æˆæ­¤é é¢æ‰€æœ‰æ›¸ç±çš„è¨ªå•
        booklinks = []
        for id in bookIDs:
            booklinks.append(generate_book_url(id))
        
        print(f"æŠ“å–åˆ°{len(bookIDs)}ç­†æ›¸æœ¬é€£çµè³‡æ–™")
        
        titles                  = []    #æ›¸åé™£åˆ— 
        ISBNs                   = []    #ISBN
        authors                 = []    #ä½œè€…
        publishs                = []    #å‡ºç‰ˆç¤¾
        dates                   = []    #å‡ºç‰ˆæ—¥æœŸ
        bookImglinks            = []    #æ›¸æœ¬å°é¢åœ–ç‰‡é€£çµ
        Successful_books_links  = []    #æˆåŠŸæŠ“åˆ°çš„æ›¸ç±é€£çµ    
        Failed_books_links      = []    #å­˜å–è¢«æ‹’çš„æ›¸ç±é€£çµ
        
        cnt = 0 #è¨ˆæ•¸å™¨ï¼Œå¯ä»¥çŸ¥é“æŠ“åˆ°çš„è³‡æ–™æ•¸
        for link in booklinks:  #é–‹å§‹ç¬¬ä¸€æ¬¡è¨ªå•æ‰€æœ‰æ›¸ç±é€£çµ
            getData_loading_bar(2,cnt+1)     #é–“éš”å…©ç§’è«‹æ±‚ä¸€æ¬¡ä»¥å…å› ç‚ºéé‡è«‹æ±‚è€Œè¢«ä¼ºæœå™¨æ‹’çµ•
            BookData = get_book_data(link)  #åˆ©ç”¨è‡ªè¨‚å‡½å¼æ±‚å¾—æ­¤å–®æœ¬æ›¸ç±çš„åŸºæœ¬è³‡æ–™
            
            if(BookData[0]!='fail'):   #æ›¸æœ¬é€£çµå­˜å–æ­£å¸¸
                if BookData[0] == 'æœªæ‰¾åˆ°è³‡æ–™':     #ç¶²è·¯ç•°å¸¸å°è‡´æŠ“å–éŒ¯èª¤ï¼Œæ”¾é€²å¤±æ•—é™£åˆ—ï¼Œä¹‹å¾Œåè¦†æª¢æŸ¥
                    print(ANSI_string('è³‡æ–™æœªæ­£ç¢ºæŠ“å–',color='red'))
                    Failed_books_links.append(link)
                else:
                    print(ANSI_string('æˆåŠŸæŠ“å–è³‡æ–™',color='green') + ANSI_string(f'ã€€{BookData}',bold=True))
                    titles.append(BookData[0])
                    ISBNs.append(BookData[1])
                    authors.append(BookData[2])
                    publishs.append(BookData[3])
                    dates.append(BookData[4])
                    bookImglinks.append(BookData[5])
                    Successful_books_links.append(link)
                    BookData = []
            else:   #æ›¸ç±é€£çµå­˜å–è¢«æ‹’ï¼Œæ”¾é€²å¤±æ•—é™£åˆ—ç­‰å¾…é‡è¤‡æª¢æŸ¥è·Ÿè«‹æ±‚
                print(ANSI_string(f'é€£æ¥å¤±æ•—ï¼ŒéŒ¯èª¤ä»£ç¢¼{BookData[1]}',color='red'))
                Failed_books_links.append(link)
            cnt=cnt+1
        df = pd.DataFrame({'æ›¸å': titles, 'æ›¸æœ¬å°é¢':bookImglinks, 'ISBN': ISBNs, 'ä½œè€…':authors, 'å‡ºç‰ˆç¤¾':publishs,'å‡ºç‰ˆæ—¥æœŸ':dates, 'æ›¸æœ¬é€£çµ': Successful_books_links})
        
        if(Failed_books_links!=[]):
            print('æŠ“å–å¤±æ•—çš„è³‡æ–™é€£çµå¦‚ä¸‹')
            df_failed = pd.DataFrame({'é€£çµ':Failed_books_links})
            print(df_failed,end="\n<====================================================>\n")
        
        fail_cnt = 1
        while Failed_books_links:   #èµ°è¨ªå¤±æ•—é™£åˆ—ï¼ŒæˆåŠŸæŠ“å–ç›®å‰ç´¢å¼•çš„è³‡æ–™å‰‡å°‡æ­¤ç´¢å¼•ä¹‹æ›¸ç±é€£çµç§»å‡ºå¤±æ•—é™£åˆ—ï¼Œç›´åˆ°å¤±æ•—é™£åˆ—è£¡çš„é€£çµéƒ½è¢«æˆåŠŸæŠ“å–
            fail_book_cnt = 1
            print('é‡æ–°æŠ“å–å¤±æ•—ä¹‹è³‡æ–™ï¼Œè«‹ç­‰å¾…5ç§’:')
            waiting_loading_bar(5)
            print(f'ç¬¬{fail_cnt}æ¬¡é‡æ–°å˜—è©¦æŠ“å–å¤±æ•—é€£çµï¼Œé‚„å‰©{len(Failed_books_links)}ç­†å¤±æ•—è³‡æ–™ï¼Œè«‹ç­‰å¾…')
            for link in Failed_books_links[:]:  # ä½¿ç”¨åˆ‡ç‰‡[:]ä»¥ä¾¿åœ¨è¿´åœˆä¸­ä¿®æ”¹åŸå§‹ä¸²åˆ—
                getData_loading_bar(2,fail_book_cnt)
                fail_book_cnt = fail_book_cnt + 1
                BookData = get_book_data(link)
                if BookData[0] != 'fail':
                    if BookData[0] == 'æœªæ‰¾åˆ°è³‡æ–™':
                        print(ANSI_string('è³‡æ–™æœªæ­£ç¢ºæŠ“å–',color='red'))
                    else:
                        print(ANSI_string('æˆåŠŸæŠ“å–è³‡æ–™',color='green') + ANSI_string(f'ã€€{BookData}',bold=True))
                        titles.append(BookData[0])
                        ISBNs.append(BookData[1])
                        authors.append(BookData[2])
                        publishs.append(BookData[3])
                        dates.append(BookData[4])
                        bookImglinks.append(BookData[5])
                        Successful_books_links.append(link)
                        Failed_books_links.remove(link)  # å¾å¤±æ•—é€£çµä¸²åˆ—ä¸­ç§»é™¤æˆåŠŸè™•ç†çš„é€£çµ
                else:
                    print(ANSI_string(f'é€£çµå¤±æ•—ï¼š{link}ï¼ŒéŒ¯èª¤ä»£ç¢¼{BookData[1]}',color='red'))
            fail_cnt = fail_cnt+1
            print("<====================================================>")
        print("æ­¤é è³‡æ–™å…¨éƒ¨æŠ“å–å®Œç•¢!")
        
        df = pd.DataFrame({'æ›¸å': titles, 'æ›¸æœ¬å°é¢':bookImglinks, 'ISBN': ISBNs, 'ä½œè€…':authors, 'å‡ºç‰ˆç¤¾':publishs,'å‡ºç‰ˆæ—¥æœŸ':dates, 'æ›¸æœ¬é€£çµ': Successful_books_links})
        return df
    else:
        print(ANSI_string(f"é€£æ¥å¤±æ•—ï¼ŒéŒ¯èª¤ä»£ç¢¼{res.status_code}",color='red'))

def generate_author_url(keyword):   #è¼¸å…¥ä½œè€…å¾Œç”¢ç”Ÿä½œè€…é é¢ä¹‹é€£çµ
    link = "https://search.books.com.tw/search/query/cat/1/v/1/adv_author/1/key/" + keyword
    print(link)
    return link

def generate_page_link(keyword,page):
    link = "https://search.books.com.tw/search/query/cat/all/sort/1/v/1/adv_author/1/spell/3/ms2/ms2_1/page/" + str(page) + "/key/" + keyword
    return link

def generate_book_url(bookID): #åˆ©ç”¨æ›¸æœ¬IDç”¢ç”Ÿè©²æ›¸é€£çµ
    return "https://www.books.com.tw/products/" + bookID + "?sloc=main"

if __name__ == "__main__":
    headers = {"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
               "AppleWebKit/537.36 (KHTML, like Gecko)"
               "Chrome/63.0.3239.132 Safari/537.36"}
    keyword=str(input("è«‹è¼¸å…¥ä½œè€…:"))
    print("å»ºç«‹é€£çµä¸­...")
    res = requests.get(generate_author_url(keyword),headers=headers)
    set_working_directory()
    # ç¾åœ¨å·¥ä½œç›®éŒ„å·²ç¶“è¨­ç½®ç‚ºåŸ·è¡Œæª”æ¡ˆæ‰€åœ¨çš„ç›®éŒ„
    print("ç•¶å‰å·¥ä½œç›®éŒ„:", os.getcwd())
    if(res.status_code == requests.codes.ok):
        content = res.content.decode()  #è§£ç¢¼ç¶²é 
        html = etree.HTML(content)
        pages_cnt_list = html.xpath('/html/body/div/div/div/div/div/ul/li/select/option/text()')
        if(pages_cnt_list!=[]):
            pages_cnt = re.search(r'\d+', pages_cnt_list[0])
            pages_cnt = int(pages_cnt.group())
        else:
            pages_cnt = 1
        print(f'å…±{pages_cnt}é ')
        df = pd.DataFrame({'æ›¸å': [], 'æ›¸æœ¬å°é¢':[], 'ISBN': [], 'ä½œè€…':[], 'å‡ºç‰ˆç¤¾':[],'å‡ºç‰ˆæ—¥æœŸ':[], 'æ›¸æœ¬é€£çµ': []})
        for i in range(1,pages_cnt+1):
            page_link = generate_page_link(keyword,i)
            print(ANSI_string(f'æŠ“å–ç¬¬{i}é è³‡æ–™ä¸­',bold=True))
            df = pd.concat([df, page_crawel(page_link)], ignore_index=True, axis=0)
        
        print('æ‰€æœ‰æ›¸ç±æŠ“å–å®Œç•¢!')
        print(df)
        set_working_directory()
        current_directory = os.getcwd()
        csv_directory = os.path.join(current_directory, "ä½œè€…csv")
        if not os.path.exists(csv_directory):
            os.makedirs(csv_directory)
        file_path = os.path.join(csv_directory, keyword + ".csv")
        df.to_csv(file_path,index=False,encoding='utf-8-sig')
        
        if(NOTION_TOKEN != "" or PAGE_ID != ""):
            upload = input('æ˜¯å¦è¦å°‡è³‡æ–™åŒ¯å…¥Notion(y/n)\n')
            if(upload=='y'):
                EstablishFullDatabase(keyword=keyword,df=df)
            else:
                print('æœªå•Ÿç”¨è‡ªå‹•ä¸Šå‚³ï¼Œå¯ä»¥ä½¿ç”¨Notionçš„åŒ¯å…¥csvåŠŸèƒ½å»ºç«‹database')
        elif(NO_secret_json):
            print(ANSI_string(s='SECRET.jsonä¸å­˜åœ¨ï¼Œè«‹ç¢ºä¿æœ‰ä½¿ç”¨SetUp.exeè¼¸å…¥æ‚¨çš„æ•´åˆå¯†ç¢¼åŠPageé€£çµ',color='red'))
        elif(NOTION_TOKEN == ""):
            print(ANSI_string(s='ç„¡æ•ˆçš„æ•´åˆå¯†ç¢¼ï¼Œè«‹ç¢ºèªé é¢æ˜¯å¦æˆåŠŸé€£çµåˆ°æ‚¨çš„integrationï¼Œæˆ–æ˜¯å†æ¬¡æª¢æŸ¥æ•´åˆå¯†ç¢¼æ˜¯å¦æœ‰èª¤',color='red'))
        elif(PAGE_ID == ""):
            print(ANSI_string(s='ç„¡æ•ˆçš„Page IDï¼Œè«‹å†æ¬¡ç¢ºèªé€£çµç„¡èª¤',color='red'))
            
        print("Press any key to exit...")
        msvcrt.getch()
    else:
        print(f'å¤±æ•—ï¼ŒéŒ¯èª¤ä»£ç¢¼{res.status_code}')
        print("Press any key to exit...")
        msvcrt.getch()